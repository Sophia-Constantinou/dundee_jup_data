{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge ProintVar files with AF seq+pred and JP seq+pred:\n",
    "\n",
    "Take merged ProIntVar files and choose columns -- save in prointvar_less \\\n",
    "Take those files and choose chain (found in SCOPe ID in Jpred data)\n",
    "\n",
    "Merge ProIntVar with Alphafold -- save in AF_Pro\n",
    "\n",
    "Add Jpred -- save in Pro_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to add AF sequence:\n",
    "\n",
    "# Open prointvar files - done\n",
    "# Assign the row to the sequence - done\n",
    "# Alignment is correct\n",
    "\n",
    "# Want to add JPred:\n",
    "\n",
    "# Open Jpred final file\n",
    "# Split each residue into its own column\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from os import path\n",
    "import re\n",
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save different versions of the same pdb's:\n",
    "\n",
    "def check_file(filePath):\n",
    "    if path.exists(filePath):\n",
    "        numb = 1\n",
    "        while True:\n",
    "            newPath = \"{0}_{2}{1}\".format(*path.splitext(filePath) + (numb,))\n",
    "            if path.exists(newPath):\n",
    "                numb += 1\n",
    "            else:\n",
    "                return newPath\n",
    "    return filePath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sophia/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (4,26,28,60,74) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/Sophia/Dundee/dundee/data/ProIntVar/prointvar_merged')\n",
    "\n",
    "x = glob.glob('*.csv')\n",
    "\n",
    "\n",
    "for file in x:\n",
    "    df = pd.read_csv(file)\n",
    "    y = df[[\n",
    "            'RES', 'CHAIN', 'AA', \n",
    "    # # #             'SS', \n",
    "            'SS_CLASS', \n",
    "                'PDB_dbResNum', 'PDB_dbResName', 'PDB_dbChainId', 'UniProt_dbAccessionId', 'UniProt_dbResNum', 'UniProt_dbResName',\n",
    "                # 'PDB_codeSecondaryStructure', 'PDB_nameSecondaryStructure']]\n",
    "    ]]\n",
    "    y.to_csv(f'/Users/Sophia/Dundee/dundee/data/ProIntVar/prointvar_less/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something's off: 3wu2\n",
      "Something's off: 4v4e\n",
      "Not a string value apparently: 2cho\n",
      "Not a string value apparently: 1gci\n",
      "Not a string value apparently: 2erl\n",
      "Not a string value apparently: 2r75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data')\n",
    "dir = os.listdir('ProIntVar/prointvar_less')\n",
    "\n",
    "for file in dir:\n",
    "    pdb = file[:4]\n",
    "    # print(pdb)\n",
    "    try:\n",
    "        df = pd.read_csv(f'ProIntVar/prointvar_less/{file}')\n",
    "        df = df.dropna()\n",
    "\n",
    "        df_scop = pd.read_csv('Jpred/jpred_final.csv')\n",
    "        df_scop = df_scop.dropna()\n",
    "\n",
    "        df_scop['PDB_ID'] = df_scop['PDB_ID'].astype(\"string\")\n",
    "\n",
    "        df_trial = df_scop[df_scop['PDB_ID'].str.contains(f'{pdb}')]\n",
    "        chain = df_trial.iloc[:, 2].str[5]\n",
    "        chain_cap = chain.str.upper()\n",
    "\n",
    "\n",
    "        df_chain = df.loc[df['PDB_dbChainId'].str.startswith(chain_cap.iloc[0])]\n",
    "\n",
    "        filename = check_file('ProIntVar/prointvar_col_chain/%s' % file)\n",
    "        df_chain.to_csv('' + filename)\n",
    "\n",
    "\n",
    "    # print(df_chain)\n",
    "\n",
    "    except AttributeError:\n",
    "        print(f'Not a string value apparently: {pdb}')\n",
    "        continue\n",
    "    except IsADirectoryError:\n",
    "        print(f'Something wrong here: {pdb}')\n",
    "        continue\n",
    "    except IndexError:\n",
    "        print(f'Something\\'s off: {pdb}')\n",
    "    # except ParserError:\n",
    "        # print(f'Something wrong here: {pdb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something wrong with: 1syx_1\n",
      "Something wrong with: 1vq8_26\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_27\n",
      "Something wrong with: 1vq8_25\n",
      "Something wrong with: 1vq8_19\n",
      "Something wrong with: 1vq8_18\n",
      "Something wrong with: 1vq8\n",
      "Something wrong with: 1vq8_24\n",
      "Something wrong with: 1gk9\n",
      "Something wrong with: 1z3e\n",
      "Something wrong with: 1vq8_20\n",
      "Something wrong with: 1vq8_21\n",
      "Something wrong with: 1nh2_2\n",
      "Something wrong with: 2jdq_1\n",
      "Something wrong with: 1vq8_23\n",
      "Something wrong with: 1z0j\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_22\n",
      "4v4e is missing\n",
      "Something wrong with: 2b59\n",
      "Something wrong with: 1tx4\n",
      "2cho is missing\n",
      "3wu2 is missing\n",
      "3wu2 is missing\n",
      "Something wrong with: 1zoq\n",
      "Something wrong with: 1n13\n",
      "Something wrong with: 1vq8_9\n",
      "Something wrong with: 1vq8_8\n",
      "Something wrong with: 1vq8_6\n",
      "3wu2 is missing\n",
      "3wu2 is missing\n",
      "Something wrong with: 1qd6\n",
      "Something wrong with: 1vq8_7\n",
      "1gci is missing\n",
      "Something wrong with: 1upt\n",
      "Something wrong with: 1vq8_5\n",
      "3wu2 is missing\n",
      "Something wrong with: 2grr_1\n",
      "2erl is missing\n",
      "Something wrong with: 1vq8_4\n",
      "3wu2 is missing\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_1\n",
      "Something wrong with: 4dri_1\n",
      "Something wrong with: 1vq8_3\n",
      "Something wrong with: 1nkp_1\n",
      "3wu2 is missing\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_2\n",
      "Something wrong with: 4j9y\n",
      "Something wrong with: 1vq8_13\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_12\n",
      "3wu2 is missing\n",
      "Something wrong with: 1qtn\n",
      "Something wrong with: 1nvu\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_10\n",
      "4v4e is missing\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_11\n",
      "Something wrong with: 1vq8_15\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_14\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_28\n",
      "Something wrong with: 2v89_1\n",
      "2r75 is missing\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_16\n",
      "3wu2 is missing\n",
      "Something wrong with: 1vq8_17\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/Sophia/Dundee/dundee/data')\n",
    "    \n",
    "    \n",
    "    \n",
    "data = pd.read_csv('AF/AF_m.csv')\n",
    "\n",
    "for i,row in data.iterrows():  \n",
    "\n",
    "    data_uni = []\n",
    "    pred = []\n",
    "    \n",
    "    pdb = row.iloc[1] \n",
    "    pdb_id = pdb[:4]\n",
    "    uniprot_seq = row.iloc[2]\n",
    "    AF_pred = row.iloc[3]\n",
    "    # print(pdb, pdb_id, uniprot_seq, AF_pred)\n",
    "    \n",
    "#     # Open Merged file - some pdb id's missing\n",
    " \n",
    "    try:\n",
    "        df3 = pd.read_csv('ProIntVar/prointvar_col_chain/%s.csv' % pdb_id)\n",
    "    except FileNotFoundError:\n",
    "        print(f'{pdb_id} is missing')\n",
    "        continue\n",
    "        \n",
    "    # Drop NaN values\n",
    "    df3_val = df3.dropna()\n",
    "#     print(df3_val, pdb_id)\n",
    "\n",
    "#   Find Uniprot offset   \n",
    "        \n",
    "    uniprot_offset = df3_val['UniProt_dbResNum']\n",
    "    df_u = pd.DataFrame(uniprot_offset)\n",
    "    df_drop = df_u.dropna()\n",
    "#     print(uniprot_offset, df_u, df_drop)\n",
    "    \n",
    "    pos_trial = []\n",
    "\n",
    "    for row in df_drop.iterrows():\n",
    "        position_pdb = row[1][0]\n",
    "#         print(position_pdb)\n",
    "        try:\n",
    "            pos = position_pdb.astype(int)\n",
    "        except ValueError:\n",
    "            print(f'Skipping {pdb_id}: PDB - Invalid data...')\n",
    "            continue        \n",
    "\n",
    "        if pos < len(uniprot_seq) and pos < len(AF_pred):\n",
    "            shifted_seq = uniprot_seq[pos-1]\n",
    "            shifted_pred = AF_pred[pos-1]\n",
    "            data_uni.append(shifted_seq)\n",
    "            pred.append(shifted_pred)\n",
    "            pos_trial.append(pos)\n",
    "\n",
    "\n",
    "    df_d = pd.DataFrame(data_uni)\n",
    "    df_pr = pd.DataFrame(pred) \n",
    "    \n",
    "\n",
    "    val_row = df3_val['UniProt_dbResNum'].first_valid_index()\n",
    "\n",
    "    df_uni = df_d.shift(periods=val_row, fill_value=0)\n",
    "    df_pr_2 = df_pr.shift(periods=val_row, fill_value=0)\n",
    "\n",
    "    try:\n",
    "        df3_val['shifted Uni'] = df_d\n",
    "        df3_val['AF_predictions'] = df_pr\n",
    "\n",
    "# #     print(df3_val, pdb_id)\n",
    "        file = check_file(f'AF_Pro/{pdb_id}.csv')\n",
    "    # Uncomment to save -- if run multiple times, saves same files with different names\n",
    "\n",
    "        df3_val.to_csv('' + file)\n",
    "    except KeyError:\n",
    "        print(f'Something wrong with: {pdb}')\n",
    "    except ValueError:\n",
    "        print(f'Something wrong with: {pdb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078\n",
      "881\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/Sophia/Dundee/dundee/data/AF_Pro')\n",
    "\n",
    "\n",
    "x = glob.glob('*.csv')\n",
    "\n",
    "pdb_id = []\n",
    "for file in x:\n",
    "    pdb = file[0:4]\n",
    "    pdb_id.append(pdb)\n",
    "  \n",
    "\n",
    "\n",
    "lol = pd.DataFrame(pdb_id)\n",
    "tsp = lol.drop_duplicates()\n",
    "print(len(x))\n",
    "print(len(tsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same to JPred\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data/Jpred')\n",
    "\n",
    "# os.chdir('/cluster/gjb_lab/2524591/data')\n",
    "\n",
    "\n",
    "data = pd.read_csv('jpred_final.csv')\n",
    "\n",
    "\n",
    "pdb_id = []\n",
    "for i,row in data.iterrows():\n",
    "    pdb = row.iloc[1]\n",
    "    pdb_id.append(pdb)\n",
    "    scop = row.iloc[2]\n",
    "        \n",
    "    df_p = pd.DataFrame(pdb_id)\n",
    "\n",
    "df_new = data['J_AA Sequence'].astype(str).apply(lambda x: pd.Series(list(x)))\n",
    "df_new.dropna()\n",
    "\n",
    "df_new['pdb_id'] = df_p\n",
    "df_new\n",
    "\n",
    "\n",
    "for i,row in df_new.iterrows():\n",
    "\n",
    "    pdb_ID = row.iloc[-1]\n",
    "    sh = row[:-1]\n",
    "    sh = sh.dropna()\n",
    "\n",
    "    file = check_file(f'J_seq/{pdb_ID}.csv')\n",
    "    try:\n",
    "        sh.to_csv('' + file)\n",
    "    except ValueError:\n",
    "        print('something wrong with: ' + pdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Do the same with predictions\n",
    "\n",
    "\n",
    "# os.chdir('/cluster/gjb_lab/2524591/data')\n",
    "\n",
    "\n",
    "data = pd.read_csv('jpred_final.csv')\n",
    "\n",
    "\n",
    "pdb = []\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    pdb_id = row.iloc[1]\n",
    "    \n",
    "    pdb.append(pdb_id)\n",
    "    \n",
    "df_p = pd.DataFrame(pdb)\n",
    "\n",
    "df_new = data['J_SS'].astype(str).apply(lambda x: pd.Series(list(x)))\n",
    "df_new.dropna()\n",
    "\n",
    "df_new['pdb_id'] = df_p\n",
    "df_new\n",
    "\n",
    "\n",
    "for i,row in df_new.iterrows():\n",
    "\n",
    "    pdb_ID = row.iloc[-1]\n",
    "    sh = row[:-1]\n",
    "\n",
    "    file = check_file(f'J_ss/{pdb_ID}.csv')\n",
    "    try:\n",
    "        sh.to_csv('' + file)\n",
    "    except ValueError:\n",
    "        print('something wrong with: ' + pdb_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Jpred prediction and ss\n",
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data/Jpred')\n",
    "\n",
    "dir = os.listdir('J_seq')\n",
    "\n",
    "# data = pd.read_csv('merged_w_AF/1rg8.csv')\n",
    "# print(data)\n",
    "\n",
    "for file in dir:\n",
    "    try:\n",
    "        df = pd.read_csv('J_seq/%s' % file)\n",
    "        df_1 = pd.read_csv('J_ss/%s' % file)\n",
    "\n",
    "        df_2 = pd.concat([df, df_1], axis=1)\n",
    "        df_2 = df_2.iloc[:, [1, 3]]\n",
    "\n",
    "        df_2.to_csv('JP/%s' % file)\n",
    "    except IsADirectoryError:\n",
    "        print(f'Invalid data: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348\n",
      "1235\n",
      "1348\n",
      "1235\n",
      "1348\n"
     ]
    }
   ],
   "source": [
    "# Check how many files and how many pdb's for JPred\n",
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data/Jpred/J_seq')\n",
    "\n",
    "x = glob.glob('*.csv')\n",
    "\n",
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data/Jpred/J_ss')\n",
    "y = glob.glob('*.csv')\n",
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data/Jpred/JP')\n",
    "z = glob.glob('*.csv')\n",
    "\n",
    "pdb_id_x = []\n",
    "pdb_y = []\n",
    "for file in x:\n",
    "    pdb = file[0:4]\n",
    "    pdb_id_x.append(pdb)\n",
    "  \n",
    "\n",
    "for file in y:\n",
    "    pdb = file[0:4]\n",
    "    pdb_y.append(pdb)\n",
    "\n",
    "\n",
    "\n",
    "lol = pd.DataFrame(pdb_id_x)\n",
    "rip = pd.DataFrame(pdb_y)\n",
    "\n",
    "tsp = lol.drop_duplicates()\n",
    "hm = rip.drop_duplicates()\n",
    "\n",
    "print(len(x))\n",
    "print(len(tsp))\n",
    "\n",
    "print(len(y))\n",
    "print(len(hm))\n",
    "\n",
    "print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JPred files merged with AF_Pro\n",
    "\n",
    "\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data')\n",
    "\n",
    "dir = os.listdir('AF_Pro')\n",
    "\n",
    "\n",
    "for file in dir:\n",
    "    try:\n",
    "        df = pd.read_csv('AF_Pro/' + file)\n",
    "        pdb = file[:4]\n",
    "        \n",
    "        file_af = check_file(f'Pro_final/{pdb}.csv')\n",
    "#         if file_af == f'/homes/2524591/data/JP_shift/Pro_final/{pdb}.csv':\n",
    "#         if not os.path.exists(f'/homes/2524591/data/JP_shift/Pro_final/{pdb}.csv'):\n",
    "        df_j = pd.read_csv('Jpred/JP/%s.csv' % pdb)\n",
    "        df_j = df_j.dropna()\n",
    "        j_sel= df_j.iloc[:, 1]\n",
    "        tsp = j_sel.iloc[0]\n",
    "        df['Jpred_seq'] = j_sel  \n",
    "\n",
    "        j_ss = df_j.iloc[:, 2]\n",
    "        hm = j_ss.iloc[0]\n",
    "        df['Jpred_SS_pred'] = j_ss\n",
    "        df = df.replace('-', 'C', regex=True)\n",
    "#         Put code to replace lowercase letters with C\n",
    "#         df = re.sub('[a-z]', 'C', df)\n",
    "        # if pdb == '1dce':\n",
    "            # print(df, file_af) \n",
    "#     Uncomment to save\n",
    "        df.to_csv(file_af) \n",
    "\n",
    "        # If there are multiple copies:\n",
    "        if os.path.exists(f'Jpred/JP/{pdb}_1.csv'):\n",
    "            file = check_file(f'Jpred/JP/{pdb}.csv')\n",
    "            x = file[-5]\n",
    "            z = int(x)\n",
    "            for i in range(1, z): #len(x)??\n",
    "                try:                   \n",
    "                    df_j = pd.read_csv(f'Jpred/JP/{pdb}_{i}.csv')\n",
    "                    df_j.dropna()\n",
    "                    j_sel= df_j.iloc[:, 1]\n",
    "                    tsp = j_sel.iloc[0]\n",
    "                    df['Jpred_seq'] = j_sel\n",
    "                                        \n",
    "                    j_ss = df_j.iloc[:, 2]\n",
    "                    hm = j_ss.iloc[0]\n",
    "                    df['Jpred_SS_pred'] = j_ss\n",
    "                    df = df.replace('-', 'C', regex=True) \n",
    "                    \n",
    "                    filee = check_file(f'Pro_final/{pdb}.csv')\n",
    "#                     Uncomment to save -- but if run multiple times, will save the same file with diff. name\n",
    "                    df.to_csv(filee)\n",
    "                except FileNotFoundError:\n",
    "                    print(f'Something wrong with: {pdb}')\n",
    "                    continue\n",
    "                    \n",
    "     \n",
    "# #     except FileNotFoundError:\n",
    "# #         print(f'{pdb} not found')\n",
    "# #         continue\n",
    "    except IsADirectoryError:\n",
    "        print(f'Invalid data: {pdb}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348 1348\n",
      "1348\n",
      "1078\n",
      "1314\n"
     ]
    }
   ],
   "source": [
    "# Look at number of files\n",
    "import glob\n",
    "os.chdir('/Users/Sophia/Dundee/dundee/data')\n",
    "\n",
    "# JPred\n",
    "x = glob.glob('Jpred/J_ss/*.csv')\n",
    "x_1 = glob.glob('Jpred/J_seq/*.csv')\n",
    "y = glob.glob('Jpred/JP/*.csv')\n",
    "\n",
    "# AF with ProIntVar\n",
    "y_1 = glob.glob('AF_Pro/*.csv')\n",
    "\n",
    "z_2 = glob.glob('Pro_final/*.csv')\n",
    "\n",
    "\n",
    "print(len(x),len(x_1))\n",
    "print(len(y))\n",
    "print(len(y_1))\n",
    "print(len(z_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27c2c3a016d0d9462be9e043c90d394f7a99c7d13fd0840df4c6855d133148d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
